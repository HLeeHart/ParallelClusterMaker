{% if base_os == 'centos7' %}
#!/usr/bin/env python3.6
{% else %}
#!/usr/bin/env python3
{% endif %}
#
################################################################################
# Name:		hashtest.py
# Author:	Rodney Marable <rodney.marable@gmail.com>
# Created On:	May 11, 2018
# Last Changed:	August 8, 2018
# Deployed On:  {{ lookup('pipe','date \"+%B %-d, %Y\"') }}
# Purpose:	Generate hashes of randomly generated byte streams
################################################################################

# Start the job timer.

import time
start_time=time.time()
datestamp = time.strftime("%S%M%H%d%m%Y")

# Import the rest of the required Python libraries.

import argparse
import binascii
import hashlib
import os
import random
import shutil
import string
import subprocess
import uuid

# Configure the parser.

parser = argparse.ArgumentParser(description='Generate hashes of randomly generated byte streams')
parser.add_argument('--count', '-c', help='Number of hashes to compute (default = 10000)', required=False, default=10000, type=int)
parser.add_argument('--byte_size', '-S', help='Size in bytes of the string to hash (default = 8192)', required=False, default=8192, type=int)
parser.add_argument('--outfile', '-O', help='Name of the hash output dump file (default = stdout)', required=False)
parser.add_argument('--archive_path', '-A', help='Path to the archive directory for storing compressed hash output dump files (default = ./archive_hashtest)', required=False, default='./archive_hashtest')
parser.add_argument('--compression-type', '-C', help='Set the compression type (default = pigz)', required=False, default='pigz')
parser.add_argument('--compression_processes', '-P', help='set the number of CPUs to use for compression', required=False, type=int, default=8)

# Set values for critical parameters based on command line input.

args = parser.parse_args()
count = args.count
byte_size = args.byte_size
if args.outfile:
    outfile = args.outfile + '.' + datestamp
archive_path = args.archive_path
compression_type = args.compression_type
compression_processes = args.compression_processes

{% if hyperthreading %}# Intel HyperThreading is enabled.
{% if compute_instance_type.split('.')[-1] == "large" %}compression_processes = 2
{% elif compute_instance_type.split('.')[-1] == "xlarge" %}compression_processes = 4
{% elif compute_instance_type.split('.')[-1] == "2xlarge" %}compression_processes = 8
{% elif compute_instance_type.split('.')[-1] == "4xlarge" %}compression_processes = 16
{% elif compute_instance_type.split('.')[-1] == "9xlarge" %}compression_processes = 36
{% elif compute_instance_type.split('.')[-1] == "12xlarge" %}compression_processes = 48
{% elif compute_instance_type.split('.')[-1] == "18xlarge" %}compression_processes = 72
{% elif compute_instance_type.split('.')[-1] == "24xlarge" %}compression_processes = 96
{% else %}#compression_processes = 4
{% endif %}
{% else %}# Intel HyperThreading is disabled.
{% if compute_instance_type.split('.')[-1] == "large" %}compression_processes = 1
{% elif compute_instance_type.split('.')[-1] == "xlarge" %}compression_processes = 2
{% elif compute_instance_type.split('.')[-1] == "2xlarge" %}compression_processes = 4
{% elif compute_instance_type.split('.')[-1] == "4xlarge" %}compression_processes = 8
{% elif compute_instance_type.split('.')[-1] == "9xlarge" %}compression_processes = 18
{% elif compute_instance_type.split('.')[-1] == "12xlarge" %}compression_processes = 24
{% elif compute_instance_type.split('.')[-1] == "18xlarge" %}compression_processes = 36
{% elif compute_instance_type.split('.')[-1] == "24xlarge" %}compression_processes = 48
{% else %}#compression_processes = 2
{% endif %}
{% endif %}

# Function: compute_test_time(t)
# Purpose: define a function to compute the elapsed test time.

def compute_test_time(t):
    end_time=t
    elapsed_time=round(end_time-start_time,4)
    if args.outfile:
        print("--------------------------------------------------------------------------------", file=open(outfile + '.hashdata_dump', "a"))
        print('',  file=open(outfile + '.hashdata_dump', "a"))
        print('archive_path = ' + archive_path + '/' + outfile + '.summary_data.gz', file=open(outfile  + '.hashdata_dump', "a"))
        print("time_elapsed = %.4f seconds  " % elapsed_time, file=open(outfile  + '.hashdata_dump', "a"))
    else:
        print('')
        print("time_elapsed  = %.4f seconds  " % elapsed_time)

# Delete any pre-existing output files to prevent clashing.

if args.outfile:
    if os.path.isfile(archive_path + '/' + outfile + '.summary_data.gz'):
        os.remove(archive_path + '/' + outfile + '.summary_data.gz')
    if os.path.isfile(archive_path + '/' + outfile + '.hashdata_dump.gz'):
        os.remove(archive_path + '/' + outfile + '.hashdata_dump.gz')
    open(outfile + '.hashdata_dump', 'w')
    open(outfile + '.summary_data', 'w')

# Generate a random string and hash it with a randomly generated salt.
# Append the salt, hash, and the random data string to output files if --outfile
# was invoked.

for i in range(1, count + 1):
    a = os.urandom(byte_size)
    salt = uuid.uuid4().hex
    if args.outfile:
        print ("count =", i, file=open(outfile + '.hashdata_dump', "a"))
        print ("input size =", byte_size, "bytes", file=open(outfile + '.hashdata_dump', "a"))
        print ("[START_DATA_BLOCK]", file=open(outfile + '.hashdata_dump', "a"))
        print (binascii.b2a_base64(a), file=open(outfile + '.hashdata_dump', "a"))
        print ("[END_DATA_BLOCK]", file=open(outfile + '.hashdata_dump', "a"))
        print ("--------------------------------------------------------------------------------", file=open(outfile + '.hashdata_dump', "a"))
        print ("count =", i, file=open(outfile + '.summary_data', "a"))
        print ("input data size =", byte_size, "bytes", file=open(outfile + '.summary_data', "a"))
        print ("salt: ", "{", salt, "}", file=open(outfile + '.summary_data', "a"))
        print ("hash: ", "{", hashlib.blake2b(salt.encode() + a).hexdigest(), "}", file=open(outfile + '.summary_data', "a"))
        print ("--------------------------------------------------------------------------------", file=open(outfile + '.summary_data', "a"))
    else:
        print ("record count #  =", i)
        print ("input file size =", byte_size, "bytes")
        print ("salt: ", "{", salt, "}")
        print ("hash: ", "{", hashlib.blake2b(salt.encode() + a).hexdigest(), "}")
        print ("--------------------------------------------------------------------------------")

# Compute the elapsed test time.
# Print the result to the console and append to outfile (if defined).

compute_test_time(time.time())

# Compress the output files and move them to archive_path.

if args.outfile:
    if not os.path.isdir(archive_path):
        os.makedirs(archive_path)
    files_to_compress = [ outfile + '.summary_data', outfile + '.hashdata_dump' ]
    for compfile in files_to_compress:
        try:
            with open(compfile) as file:
                if (compression_type) == "pigz":
                    cmd_string = 'pigz -f --processes ' + str(compression_processes)  + ' ' + compfile
                    subprocess.run(str(cmd_string), shell=True)
                if (compression_type) == "gzip":
                    f_in=open(compfile, 'rb')
                    f_out=gzip.open(compfile + '.gz', 'wb')
                    shutil.copyfileobj(f_in, f_out)
        except IOError:
            pass
    for compfile in files_to_compress:
        shutil.move(compfile + '.gz', archive_path + '/' + compfile + '.gz')
