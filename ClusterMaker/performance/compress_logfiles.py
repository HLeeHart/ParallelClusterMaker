#!/usr/bin/env python3
#
################################################################################
# Name:         compress_logfiles.py
# Author:       Rodney Marable <rodney.marable@gmail.com>
# Created On:   April 20, 2018
# Last Changed: April 24, 2018
# Deployed On:  {{ lookup('pipe','date \"+%B %-d, %Y\"') }}
# Purpose:      Compress log files generated by Axb_random.py
#
# Usage:
# $ compress_logfiles.py [-h] --jobid JOBID --matrix-size MATRIX_SIZE
################################################################################

# Start the job timer.

import time
start_time=time.time()

# Import the rest of the required Python libraries.

import argparse
import contextlib
import gzip 
import os
import platform
import shutil
import subprocess
import sys

# Get the testing instance hostname and the current working directory.

exec_node = platform.uname()[1]
cwd = os.getcwd()

# Parse values for cluster_jobid and matrix_size from the command line.

parser = argparse.ArgumentParser(description='Compress log files generated by Axb_random.py')
parser.add_argument('--jobid', '-J', help='name of the job - used to determine CONSOLE_DUMP, CONSOLE_LOG, and CSV_DATA file names', required=True)
parser.add_argument('--matrix-size', '-M', help='set dimensions of the square matrix A', required=True, type=int)
parser.add_argument('--compression_type', '-C', help='set the file compression type', required=False, default='gzip')
parser.add_argument('--compression_processes', '-P', help='set the number of CPUs to use for compression', required=False, default='8')

args = parser.parse_args()
cluster_jobid = args.jobid
matrix_size = args.matrix_size
compression_type = args.compression_type
compression_processes = args.compression_processes

# Start a new timer to measure logfile compression activities.

start_fileproc_time=time.time()

# Capture the size of the uncompressed logfile.

raw_log_size_bytes = os.path.getsize(cluster_jobid + ".log")

# Compress the logfile with gzip or pigz.

print("+ Initiated log file compression with", compression_type, "on %s " % time.strftime("%a %d %b %Y @ %H:%M:%S", time.localtime(start_fileproc_time)))
if (compression_type) == "gzip":
    f_in=open(cluster_jobid + '.log', 'rb')
    f_out=gzip.open(cluster_jobid + '.log.gz', 'wb')
    shutil.copyfileobj(f_in, f_out)
if (compression_type) == "pigz":
    subprocess.run(["pigz", "-f", "--processes", compression_processes, cluster_jobid + '.log'])


# Record the size of the compressed logfile.
# Move the compressed logfile to the logs/ directory.
# Delete the uncompressed logfile.

compressed_log_size_bytes = os.path.getsize(cluster_jobid + ".log.gz")
shutil.move(cluster_jobid + '.log.gz', 'logs/' + cluster_jobid + '.' + str(matrix_size) + '.log.gz')
with contextlib.suppress(FileNotFoundError):
    os.remove(cluster_jobid + '.log')

# Stop the file compression timer.

end_fileproc_time=time.time()

# Save the timing results and logfile sizes to a CSV file.

print("+ Completed log file compression with", compression_type, "on %s " % time.strftime("%a %d %b %Y @ %H:%M:%S", time.localtime(end_fileproc_time)))
elapsed_fileproc_time=round(end_fileproc_time-start_fileproc_time,4)
print(raw_log_size_bytes, ',', compressed_log_size_bytes, ',', elapsed_fileproc_time, sep='', file=open(cluster_jobid + ".time.csv", "w"))

# Cleanup and exit.

sys.exit(0)
